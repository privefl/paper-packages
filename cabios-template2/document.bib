@article{Abraham2012,
abstract = {ABSTRACT: BACKGROUND: A central goal of genomics is to predict phenotypic variation from genetic variation. Fitting predictive models to genome-wide and whole genome single nucleotide polymorphism (SNP) profiles allows us to estimate the predictive power of the SNPs and potentially develop diagnostic models for disease. However, many current datasets cannot be analysed with standard tools due to their large size. RESULTS: We introduce SparSNP, a tool for fitting lasso linear models for massive SNP datasets quickly and with very low memory requirements. In analysis on a large celiac disease case/control dataset, we show that SparSNP runs substantially faster than four other state-of-the-art tools for fitting large scale penalised models. SparSNP was one of only two tools that could successfully fit models to the entire celiac disease dataset, and it did so with superior performance. Compared with the other tools, the models generated by SparSNP had better than or equal to predictive performance in cross-validation. CONCLUSIONS: Genomic datasets are rapidly increasing in size, rendering existing approaches to model fitting impractical due to their prohibitive time or memory requirements. This study shows that SparSNP is an essential addition to the genomic analysis toolkit.SparSNP is available at http://www.genomics.csse.unimelb.edu.au/SparSNP.},
author = {Abraham, Gad and Kowalczyk, Adam and Zobel, Justin and Inouye, Michael},
doi = {10.1186/1471-2105-13-88},
file = {:home/privef/Bureau/thesis/articles/Abraham et al. - 2012 - SparSNP Fast and memory-efficient analysis of all SNPs for phenotype prediction.pdf:pdf},
isbn = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {88},
pmid = {22574887},
title = {{SparSNP: Fast and memory-efficient analysis of all SNPs for phenotype prediction}},
volume = {13},
year = {2012}
}
@article{Wang2012,
abstract = {BACKGROUND: Single nucleotide polymorphism (SNP) genotyping assays normally give rise to certain percents of no-calls; the problem becomes severe when the target organisms, such as cattle, do not have a high resolution genomic sequence. Missing SNP genotypes, when related to target traits, would confound downstream data analyses such as genome-wide association studies (GWAS). Existing methods for recovering the missing values are successful to some extent - either accurate but not fast enough or fast but not accurate enough.

RESULTS: To a target missing genotype, we take only the SNP loci within a genetic distance vicinity and only the samples within a similarity vicinity into our local imputation process. For missing genotype imputation, the comparative performance evaluations through extensive simulation studies using real human and cattle genotype datasets demonstrated that our nearest neighbor based local imputation method was one of the most efficient methods, and outperformed existing methods except the time-consuming fastPHASE; for missing haplotype allele imputation, the comparative performance evaluations using real mouse haplotype datasets demonstrated that our method was not only one of the most efficient methods, but also one of the most accurate methods.

CONCLUSIONS: Given that fastPHASE requires a long imputation time on medium to high density datasets, and that our nearest neighbor based local imputation method only performed slightly worse, yet better than all other methods, one might want to adopt our method as an alternative missing SNP genotype or missing haplotype allele imputation method.},
author = {Wang, Yining and Cai, Zhipeng and Stothard, Paul and Moore, Steve and Goebel, Randy and Wang, Lusheng and Lin, Guohui},
doi = {10.1186/1756-0500-5-404},
file = {:home/privef/Bureau/thesis/articles/Wang et al. - 2012 - Fast accurate missing SNP genotype local imputation.pdf:pdf},
issn = {1756-0500},
journal = {BMC research notes},
keywords = {Alleles,Animals,Cattle,Genome-Wide Association Study,Genotype,Haplotypes,Humans,Mice,Polymorphism, Single Nucleotide,Reproducibility of Results},
language = {En},
month = {jan},
number = {1},
pages = {404},
pmid = {22863359},
publisher = {BioMed Central},
title = {{Fast accurate missing SNP genotype local imputation.}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/1756-0500-5-404},
volume = {5},
year = {2012}
}
@article{Palmer2016,
abstract = {Missing data are an unavoidable component of modern statistical genetics. Different array or sequencing technologies cover different single nucleotide polymorphisms (SNPs), leading to a complicated mosaic pattern of missingness where both individual genotypes and entire SNPs are sporadically absent. Such missing data patterns cannot be ignored without introducing bias, yet cannot be inferred exclusively from nonmissing data. In genome-wide association studies, the accepted solution to missingness is to impute missing data using external reference haplotypes. The resulting probabilistic genotypes may be analyzed in the place of genotype calls. A general-purpose paradigm, called Multiple Imputation (MI), is known to model uncertainty in many contexts, yet it is not widely used in association studies. Here, we undertake a systematic evaluation of existing imputed data analysis methods and MI. We characterize biases related to uncertainty in association studies, and find that bias is introduced both at the imputation level, when imputation algorithms generate inconsistent genotype probabilities, and at the association level, when analysis methods inadequately model genotype uncertainty. We find that MI performs at least as well as existing methods or in some cases much better, and provides a straightforward paradigm for adapting existing genotype association methods to uncertain data.},
author = {Palmer, Cameron and Pe'er, Itsik},
doi = {10.1371/journal.pgen.1006091},
editor = {Browning, Sharon},
file = {:home/privef/Bureau/thesis/articles/Palmer, Peer - 2016 - Bias Characterization in Probabilistic Genotype Data and Improved Signal Detection with Multiple Imputation.pdf:pdf},
issn = {15537404},
journal = {PLoS Genetics},
month = {jun},
number = {6},
pages = {e1006091},
pmid = {27310603},
publisher = {Wiley-Interscience},
title = {{Bias Characterization in Probabilistic Genotype Data and Improved Signal Detection with Multiple Imputation}},
url = {http://dx.plos.org/10.1371/journal.pgen.1006091},
volume = {12},
year = {2016}
}
@article{Dubois2010,
abstract = {We performed a second-generation genome-wide association study of 4,533 individuals with celiac disease (cases) and 10,750 control subjects. We genotyped 113 selected SNPs with P(GWAS) {\textless} 10(-4) and 18 SNPs from 14 known loci in a further 4,918 cases and 5,684 controls. Variants from 13 new regions reached genome-wide significance (P(combined) {\textless} 5 x 10(-8)); most contain genes with immune functions (BACH2, CCR4, CD80, CIITA-SOCS1-CLEC16A, ICOSLG and ZMIZ1), with ETS1, RUNX3, THEMIS and TNFRSF14 having key roles in thymic T-cell selection. There was evidence to suggest associations for a further 13 regions. In an expression quantitative trait meta-analysis of 1,469 whole blood samples, 20 of 38 (52.6{\%}) tested loci had celiac risk variants correlated (P {\textless} 0.0028, FDR 5{\%}) with cis gene expression.},
author = {Dubois, Patrick C A and Trynka, Gosia and Franke, Lude and Hunt, Karen A and Romanos, Jihane and Curtotti, Alessandra and Zhernakova, Alexandra and Heap, Graham A R and Ad{\'{a}}ny, R{\'{o}}za and Aromaa, Arpo and Bardella, Maria Teresa and van den Berg, Leonard H and Bockett, Nicholas A and de la Concha, Emilio G and Dema, B{\'{a}}rbara and Fehrmann, Rudolf S N and Fern{\'{a}}ndez-Arquero, Miguel and Fiatal, Szilvia and Grandone, Elvira and Green, Peter M and Groen, Harry J M and Gwilliam, Rhian and Houwen, Roderick H J and Hunt, Sarah E and Kaukinen, Katri and Kelleher, Dermot and Korponay-Szabo, Ilma and Kurppa, Kalle and MacMathuna, Padraic and M{\"{a}}ki, Markku and Mazzilli, Maria Cristina and McCann, Owen T and Mearin, M Luisa and Mein, Charles A and Mirza, Muddassar M and Mistry, Vanisha and Mora, Barbara and Morley, Katherine I and Mulder, Chris J and Murray, Joseph A and N{\'{u}}{\~{n}}ez, Concepci{\'{o}}n and Oosterom, Elvira and Ophoff, Roel A and Polanco, Isabel and Peltonen, Leena and Platteel, Mathieu and Rybak, Anna and Salomaa, Veikko and Schweizer, Joachim J and Sperandeo, Maria Pia and Tack, Greetje J and Turner, Graham and Veldink, Jan H and Verbeek, Wieke H M and Weersma, Rinse K and Wolters, Victorien M and Urcelay, Elena and Cukrowska, Bozena and Greco, Luigi and Neuhausen, Susan L and McManus, Ross and Barisani, Donatella and Deloukas, Panos and Barrett, Jeffrey C and Saavalainen, Paivi and Wijmenga, Cisca and van Heel, David A},
doi = {10.1038/ng.543},
file = {:home/privef/Bureau/thesis/articles/Dubois et al. - 2010 - Multiple common variants for celiac disease influencing immune gene expression.pdf:pdf},
isbn = {1546-1718 (Electronic)$\backslash$r1061-4036 (Linking)},
issn = {1546-1718},
journal = {Nature genetics},
keywords = {Case-Control Studies,Celiac Disease,Celiac Disease: genetics,Gene Expression,Gene Expression Profiling,Genes, MHC Class I,Genome-Wide Association Study,Humans,Meta-Analysis as Topic,Polymorphism, Single Nucleotide,Risk},
language = {en},
month = {apr},
number = {4},
pages = {295--302},
pmid = {20190752},
publisher = {Nature Publishing Group},
title = {{Multiple common variants for celiac disease influencing immune gene expression.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2847618{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {42},
year = {2010}
}
@article{Nelson2008,
abstract = {Technological and scientific advances, stemming in large part from the Human Genome and HapMap projects, have made large-scale, genome-wide investigations feasible and cost effective. These advances have the potential to dramatically impact drug discovery and development by identifying genetic factors that contribute to variation in disease risk as well as drug pharmacokinetics, treatment efficacy, and adverse drug reactions. In spite of the technological advancements, successful application in biomedical research would be limited without access to suitable sample collections. To facilitate exploratory genetics research, we have assembled a DNA resource from a large number of subjects participating in multiple studies throughout the world. This growing resource was initially genotyped with a commercially available genome-wide 500,000 single-nucleotide polymorphism panel. This project includes nearly 6,000 subjects of African-American, East Asian, South Asian, Mexican, and European origin. Seven informative axes of variation identified via principal-component analysis (PCA) of these data confirm the overall integrity of the data and highlight important features of the genetic structure of diverse populations. The potential value of such extensively genotyped collections is illustrated by selection of genetically matched population controls in a genome-wide analysis of abacavir-associated hypersensitivity reaction. We find that matching based on country of origin, identity-by-state distance, and multidimensional PCA do similarly well to control the type I error rate. The genotype and demographic data from this reference sample are freely available through the NCBI database of Genotypes and Phenotypes (dbGaP). {\textcopyright} 2008 The American Society of Human Genetics.},
author = {Nelson, Matthew R and Bryc, Katarzyna and King, Karen S and Indap, Amit and Boyko, Adam R and Novembre, John and Briley, Linda P and Maruyama, Yuka and Waterworth, Dawn M and Waeber, G{\'{e}}rard and Vollenweider, Peter and Oksenberg, Jorge R and Hauser, Stephen L and Stirnadel, Heide A and Kooner, Jaspal S and Chambers, John C and Jones, Brendan and Mooser, Vincent and Bustamante, Carlos D and Roses, Allen D and Burns, Daniel K and Ehm, Margaret G and Lai, Eric H},
doi = {10.1016/j.ajhg.2008.08.005},
file = {:home/privef/Bureau/thesis/articles/Nelson et al. - 2008 - The Population Reference Sample, POPRES A Resource for Population, Disease, and Pharmacological Genetics Research.pdf:pdf},
isbn = {1537-6605 (Electronic)$\backslash$r0002-9297 (Linking)},
issn = {00029297},
journal = {American Journal of Human Genetics},
month = {sep},
number = {3},
pages = {347--358},
pmid = {18760391},
publisher = {Elsevier},
title = {{The Population Reference Sample, POPRES: A Resource for Population, Disease, and Pharmacological Genetics Research}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18760391 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2556436},
volume = {83},
year = {2008}
}
@article{Tibshirani2012,
abstract = {We consider rules for discarding predictors in lasso regression and related problems, for computational efficiency. El Ghaoui and his colleagues have propose 'SAFE' rules, based on univariate inner products between each predictor and the outcome, which guarantee that a coefficient will be 0 in the solution vector. This provides a reduction in the number of variables that need to be entered into the optimization. We propose strong rules that are very simple and yet screen out far more predictors than the SAFE rules. This great practical improvement comes at a price: the strong rules are not foolproof and can mistakenly discard active predictors, i.e. predictors that have non-zero coefficients in the solution. We therefore combine them with simple checks of the Karush-Kuhn-Tucker conditions to ensure that the exact solution to the convex problem is delivered. Of course, any (approximate) screening method can be combined with the Karush-Kuhn-Tucker, conditions to ensure the exact solution; the strength of the strong rules lies in the fact that, in practice, they discard a very large number of the inactive predictors and almost never commit mistakes. We also derive conditions under which they are foolproof. Strong rules provide substantial savings in computational time for a variety of statistical optimization problems.},
author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan J},
doi = {10.1111/j.1467-9868.2011.01004.x},
file = {:home/privef/Bureau/thesis/articles/Tibshirani et al. - 2012 - Strong rules for discarding predictors in lasso-type problems.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
month = {mar},
number = {2},
pages = {245--266},
pmid = {25506256},
title = {{Strong rules for discarding predictors in lasso-type problems.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4262615{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {74},
year = {2012}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
archivePrefix = {arXiv},
arxivId = {NIHMS201118},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.1359/JBMR.0301229},
eprint = {NIHMS201118},
file = {:home/privef/Bureau/thesis/articles/Friedman, Hastie, Tibshirani - 2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
isbn = {096368910X},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--22},
pmid = {20808728},
publisher = {NIH Public Access},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20808728 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2929880 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2929880{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {33},
year = {2010}
}
@article{Zou2005,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n).By contrast, the lasso is not a very satisfactory variable selection method in the p{\textgreater}{\textgreater}n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:home/privef/Bureau/thesis/articles/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
month = {apr},
number = {2},
pages = {301--320},
pmid = {20713001},
publisher = {Blackwell Publishing Ltd},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1996}
}
@article{Golan2014,
abstract = {For predicting genetic risk, we propose a statistical approach that is specifically adapted to dealing with the challenges imposed by disease phenotypes and case-control sampling. Our approach (termed Genetic Risk Scores Inference [GeRSI]), combines the power of fixed-effects models (which estimate and aggregate the effects of single SNPs) and random-effects models (which rely primarily on whole-genome similarities between individuals) within the framework of the widely used liability-threshold model. We demonstrate in extensive simulation that GeRSI produces predictions that are consistently superior to current state-of-the-art approaches. When applying GeRSI to seven phenotypes from the Wellcome Trust Case Control Consortium (WTCCC) study, we confirm that the use of random effects is most beneficial for diseases that are known to be highly polygenic: hypertension (HT) and bipolar disorder (BD). For HT, there are no significant associations in the WTCCC data. The fixed-effects model yields an area under the ROC curve (AUC) of 54{\{}{\%}{\}}, whereas GeRSI improves it to 59{\{}{\%}{\}}. For BD, using GeRSI improves the AUC from 55{\{}{\%}{\}} to 62{\{}{\%}{\}}. For individuals ranked at the top 10{\{}{\%}{\}} of BD risk predictions, using GeRSI substantially increases the BD relative risk from 1.4 to 2.5.},
archivePrefix = {arXiv},
arxivId = {1405.2709},
author = {Golan, David and Rosset, Saharon},
doi = {10.1016/j.ajhg.2014.09.007},
eprint = {1405.2709},
file = {:home/privef/Bureau/thesis/articles/Golan, Rosset - 2014 - Effective genetic-risk prediction using mixed models.pdf:pdf},
issn = {15376605},
journal = {American Journal of Human Genetics},
number = {4},
pages = {383--393},
pmid = {25279982},
publisher = {The American Society of Human Genetics},
title = {{Effective genetic-risk prediction using mixed models}},
url = {http://dx.doi.org/10.1016/j.ajhg.2014.09.007},
volume = {95},
year = {2014}
}
@article{Dudbridge2013,
abstract = {Polygenic scores have recently been used to summarise genetic effects among an ensemble of markers that do not individually achieve significance in a large-scale association study. Markers are selected using an initial training sample and used to construct a score in an independent replication sample by forming the weighted sum of associated alleles within each subject. Association between a trait and this composite score implies that a genetic signal is present among the selected markers, and the score can then be used for prediction of individual trait values. This approach has been used to obtain evidence of a genetic effect when no single markers are significant, to establish a common genetic basis for related disorders, and to construct risk prediction models. In some cases, however, the desired association or prediction has not been achieved. Here, the power and predictive accuracy of a polygenic score are derived from a quantitative genetics model as a function of the sizes of the two samples, explained genetic variance, selection thresholds for including a marker in the score, and methods for weighting effect sizes in the score. Expressions are derived for quantitative and discrete traits, the latter allowing for case/control sampling. A novel approach to estimating the variance explained by a marker panel is also proposed. It is shown that published studies with significant association of polygenic scores have been well powered, whereas those with negative results can be explained by low sample size. It is also shown that useful levels of prediction may only be approached when predictors are estimated from very large samples, up to an order of magnitude greater than currently available. Therefore, polygenic scores currently have more utility for association testing than predicting complex traits, but prediction will become more feasible as sample sizes continue to grow.},
author = {Dudbridge, Frank},
doi = {10.1371/journal.pgen.1003348},
isbn = {1553-7404},
issn = {15537390},
journal = {PLoS Genetics},
number = {3},
pmid = {23555274},
title = {{Power and Predictive Accuracy of Polygenic Risk Scores}},
volume = {9},
year = {2013}
}
@article{Chatterjee2013,
abstract = {We report a new method to estimate the predictive performance of polygenic models for risk prediction and assess predictive performance for ten complex traits or common diseases. Using estimates of effect-size distribution and heritability derived from current studies, we project that although 45{\%} of the variance of height has been attributed to SNPs, a model trained on one million people may only explain 33.4{\%} of variance of the trait. Models based on current studies allow for identification of 3.0{\%}, 1.1{\%} and 7.0{\%} of the populations at twofold or higher than average risk for type 2 diabetes, coronary artery disease and prostate cancer, respectively. Tripling of sample sizes could elevate these percentages to 18.8{\%}, 6.1{\%} and 12.2{\%}, respectively. The utility of polygenic models for risk prediction will depend on achievable sample sizes for the training data set, the underlying genetic architecture and the inclusion of information on other risk factors, including family history.},
author = {Chatterjee, Nilanjan and Wheeler, Bill and Sampson, Joshua and Hartge, Patricia and Chanock, Stephen J and Park, Ju-Hyun},
doi = {10.1038/ng.2579},
isbn = {1546-1718 (Electronic)$\backslash$n1061-4036 (Linking)},
issn = {1546-1718},
journal = {Nature genetics},
keywords = {Algorithms,Disease,Disease: genetics,Female,Genetic,Genome-Wide Association Study,Humans,Models,Multifactorial Inheritance,Multifactorial Inheritance: genetics,Risk Assessment,Risk Factors,Statistical},
month = {mar},
number = {4},
pages = {400--5, 405e1--3},
pmid = {23455638},
title = {{Projecting the performance of risk prediction based on polygenic analyses of genome-wide association studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23455638 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3729116 http://www.nature.com/doifinder/10.1038/ng.2579 http://dx.doi.org/10.1038/ng.2579{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3729116{\&}tool=pmcentrez{\&}rendertype=abstract{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23455638{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/23455638{\%}5Cnhttp://www.pubmedcentral.nih.g},
volume = {45},
year = {2013}
}
@book{Zheng2012,
address = {Boston, MA},
author = {Zheng, Gang and Yang, Yaning and Zhu, Xiaofeng and Elston, Robert C.},
doi = {10.1007/978-1-4614-2245-7},
isbn = {978-1-4614-2244-0},
publisher = {Springer US},
series = {Statistics for Biology and Health},
title = {{Analysis of Genetic Association Studies}},
url = {http://link.springer.com/10.1007/978-1-4614-2245-7},
year = {2012}
}
@inproceedings{Luu2017,
abstract = {We introduce the R package pcadapt that performs genome scans to detect genes under selection based on population genomic data. The statistical method implemented in pcadapt assumes that markers excessively related with population structure are candidates for local adaptation. Because population structure is ascertained with principal component analysis (PCA), the package is fast and can handle large-scale data generated with next-generation technologies. It can also handle missing data as well as data obtained from pooled sequencing. By contrast to population-based approaches, the package can handle admixed individuals and does not require to group individuals into predefined populations. Using data simulated under an island model, a divergence model and range expansion, we compare pcadapt to other software performing genome scans (BayeScan, hapflk, OutFLANK, sNMF). For the different software, the average proportion of false discoveries is around the nominal false discovery rate set at 10{\%} with the exception of BayeScan that generates 40{\%} of false discoveries. When comparing statistical power for a realized percentage of false discoveries, we find that the power of BayeScan can be severely impacted by the presence of admixed individuals whereas pcadapt is not impacted. Last, we show that pcadapt is the most powerful method in a model of range expansion where population structure is continuous. Because pcadapt can handle molecular data generated with next sequencing technologies, we anticipate that it will be a valuable tool for modern analysis in molecular ecology.},
author = {Luu, Keurcien and Bazin, Eric and Blum, Michael G. B.},
booktitle = {Molecular Ecology Resources},
doi = {10.1111/1755-0998.12592},
file = {:home/privef/Bureau/thesis/articles/Luu, Bazin, Blum - 2017 - pcadapt an R package to perform genome scans for selection based on principal component analysis.pdf:pdf},
isbn = {1755-0998 (Electronic) 1755-098X (Linking)},
issn = {17550998},
keywords = {Mahalanobis distance,R package,outlier detection,population genetics,principal component analysis},
month = {jan},
number = {1},
pages = {67--77},
pmid = {27601374},
title = {{pcadapt: an R package to perform genome scans for selection based on principal component analysis}},
url = {http://doi.wiley.com/10.1111/1755-0998.12592},
volume = {17},
year = {2017}
}
@article{Abdellaoui2013,
abstract = {Genetic variation in a population can be summarized through principal component analysis (PCA) on genome-wide data. PCs derived from such analyses are valuable for genetic association studies, where they can correct for population stratification. We investigated how to capture the genetic population structure in a well-characterized sample from the Netherlands and in a worldwide data set and examined whether (1) removing long-range linkage disequilibrium (LD) regions and LD-based SNP pruning significantly improves correlations between PCs and geography and (2) whether genetic differentiation may have been influenced by migration and/or selection. In the Netherlands, three PCs showed significant correlations with geography, distinguishing between: (1) North and South; (2) East and West; and (3) the middle-band and the rest of the country. The third PC only emerged with minimized LD, which also significantly increased correlations with geography for the other two PCs. In addition to geography, the Dutch North–South PC showed correlations with genome-wide homozygosity (r ¼ 0.245), which may reflect a serial-founder effect due to northwards migration, and also with height ({\#}: r ¼ 0.142, {\~{}}: r ¼ 0.153). The divergence between subpopulations identified by PCs is partly driven by selection pressures. The first three PCs showed significant signals for diversifying selection (545 SNPs -the majority within 184 genes). The strongest signal was observed between North and South for the functional SNP in HERC2 that determines human blue/brown eye color. Thus, this study demonstrates how to increase ancestry signals in a relatively homogeneous population and how those signals can reveal evolutionary history.},
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608246v3},
author = {Abdellaoui, Abdel and Hottenga, Jouke-Jan and {De Knijff}, Peter and Nivard, Michel G and Xiao, Xiangjun and Scheet, Paul and Brooks, Andrew and Ehli, Erik A and Hu, Yueshan and Davies, Gareth E and Hudziak, James J and Sullivan, Patrick F and {Van Beijsterveldt}, Toos and Willemsen, Gonneke and {De Geus}, Eco J and Penninx, Brenda Wjh and Boomsma, Dorret I},
doi = {10.1038/ejhg.2013.48},
eprint = {0608246v3},
file = {:home/privef/Bureau/thesis/articles/Abdellaoui et al. - 2013 - Population structure, migration, and diversifying selection in the Netherlands.pdf:pdf},
isbn = {1018-4813},
issn = {1476-5438},
journal = {European Journal of Human Genetics},
keywords = {Netherlands,PCA,diversifying selection,linkage disequilibrium,migration,population structure},
month = {nov},
number = {10},
pages = {1277--1285},
pmid = {23531865},
primaryClass = {arXiv:physics},
publisher = {Nature Publishing Group},
title = {{Population structure, migration, and diversifying selection in the Netherlands}},
url = {http://www.nature.com/doifinder/10.1038/ejhg.2013.48},
volume = {21},
year = {2013}
}
@misc{Price2008,
abstract = {To the Editor: In the September 2007 issue of The Journal, Tang et al. analyzed data from 192 Puerto Ricans geno- typed at 112,584 autosomal markers and identified three regions with a deficiency in the proportion of European ancestry. They concluded that recent selection occurred at these regions after the admixture of European, African, and Native American ancestors.1 These signals of selection are very strong:We estimate that they each correspond to selection coefficients of {\textgreater}0.08 per generation, which if confirmed would represent the three most powerful selec- tive adaptations discovered to date in humans. Here, we demonstrate that on the basis of the method the authors applied, these signals of selection could be explained as ar- tifacts of the unusual long-range linkage disequilibrium (LD) that occurs at these regions and that is not specific to Puerto Ricans.We failed to replicate the signal of selec- tion in an independent and larger study of 364 Puerto Ri- can samples, when we applied a method that is not suscep- tible to this confounder. Our results highlight a complexity in the analysis of dense genotype data from recently admixed populations; this complexity needs to be taken into account not only in genome-wide screens for selec- tion but also in genome-wide association studies to ensure that false-positive signals are avoided.},
author = {Price, Alkes L. and Weale, Michael E. and Patterson, Nick and Myers, Simon R. and Need, Anna C. and Shianna, Kevin V. and Ge, Dongliang and Rotter, Jerome I. and Torres, Esther and Taylor, Kent D D. and Goldstein, David B. and Reich, David},
booktitle = {American Journal of Human Genetics},
doi = {10.1016/j.ajhg.2008.06.005},
file = {:home/privef/Bureau/thesis/articles/Price et al. - 2008 - Long-Range LD Can Confound Genome Scans in Admixed Populations.pdf:pdf},
isbn = {0002-9297},
issn = {00029297},
number = {1},
pages = {132--135},
pmid = {18606306},
title = {{Long-Range LD Can Confound Genome Scans in Admixed Populations}},
volume = {83},
year = {2008}
}
@article{Lehoucq1996,
abstract = {A deflation procedure is introduced that is designed to improve the convergence of an implicitly restarted Arnoldi iteration for computing a few eigenvalues of a large matrix. As the iteration progresses, the Ritz value approximations of the eigenvalues converge at different rates. A numerically stable scheme is introduced that implicitly deflates the converged approximations from the iteration. We present two forms of implicit deflation. The first, a locking operation, decouples converged Ritz values and associated vectors from the active part of the iteration. The second, a purging operation, removes unwanted but converged Ritz pairs. Convergence of the iteration is improved and a reduction in computational effort is also achieved. The deflation strategies make it possible to compute multiple or clustered eigenvalues with a single vector restart method. A block method is not required. These schemes are analyzed with respect to numerical stability, and computational results are presented.},
author = {Lehoucq, Rich Bruno and Sorensen, D. C.},
doi = {10.1137/S0895479895281484},
file = {:home/privef/Bureau/thesis/articles/Lehoucq, Sorensen - 1996 - Deflation Techniques for an Implicitly Restarted Arnoldi Iteration.pdf:pdf},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {1,65f15,65g05,ams subject classi cations,an e cient procedure,arnoldi method,de ation,eigenvalues,for approximat-,implicit restarting,ing a subset of,introduction,lanczos method,large sparse n n,matrix a,the arnoldi method,the arnoldi method is,the eigensystem of a},
month = {oct},
number = {4},
pages = {789--821},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Deflation Techniques for an Implicitly Restarted Arnoldi Iteration}},
url = {http://epubs.siam.org/doi/10.1137/S0895479895281484},
volume = {17},
year = {1996}
}
@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:home/privef/Bureau/thesis/articles/Chen, Guestrin - 2016 - XGBoost Reliable Large-scale Tree Boosting System.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
journal = {arXiv},
keywords = {large-scale machine learning},
month = {mar},
pages = {1--6},
pmid = {22942019},
title = {{XGBoost : Reliable Large-scale Tree Boosting System}},
url = {http://arxiv.org/abs/1603.02754 http://dx.doi.org/10.1145/2939672.2939785},
year = {2016}
}
@article{Browning2009,
abstract = {We present methods for imputing data for ungenotyped markers and for inferring haplotype phase in large data sets of unrelated individuals and parent-offspring trios. Our methods make use of known haplotype phase when it is available, and our methods are computationally efficient so that the full information in large reference panels with thousands of individuals is utilized. We demonstrate that substantial gains in imputation accuracy accrue with increasingly large reference panel sizes, particularly when imputing low-frequency variants, and that unphased reference panels can provide highly accurate genotype imputation.We place our methodology in a unified framework that enables the simultaneous use of unphased and phased data from trios and unrelated individuals in a single analysis. For unrelated individuals, our imputation methods produce well-calibrated posterior genotype probabilities and highly accurate allelefrequency estimates. For trios, our haplotype-inference method is four orders of magnitude faster than the gold-standard PHASE program and has excellent accuracy. Our methods enable genotype imputation to be performed with unphased trio or unrelated reference panels, thus accounting for haplotype-phase uncertainty in the reference panel.We present a useful measure of imputation accuracy, allelic R2, and show that this measure can be estimated accurately from posterior genotype probabilities. Our methods are implemented in version 3.0 of the BEAGLE software package. ?? 2009 by The American Society of Human Genetics. All rights reserved.},
author = {Browning, Brian L. and Browning, Sharon R.},
doi = {10.1016/j.ajhg.2009.01.005},
isbn = {1537-6605 (Electronic)$\backslash$r0002-9297 (Linking)},
issn = {00029297},
journal = {American Journal of Human Genetics},
month = {feb},
number = {2},
pages = {210--223},
pmid = {19200528},
title = {{A unified approach to genotype imputation and haplotype-phase inference for large data sets of trios and unrelated individuals}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0002929709000123},
volume = {84},
year = {2008}
}
@Manual{RSpectra2016,
    title = {RSpectra: Solvers for Large Scale Eigenvalue and SVD Problems},
    author = {Yixuan Qiu and Jiali Mei},
    year = {2016},
    note = {R package version 0.12-0},
    url = {https://CRAN.R-project.org/package=RSpectra},
  }
@article{Zeng2017,
abstract = {Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like glmnet and ncvreg are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called biglasso that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 31 GB real data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages.},
archivePrefix = {arXiv},
arxivId = {1701.05936},
author = {Zeng, Yaohui and Breheny, Patrick},
eprint = {1701.05936},
file = {:home/privef/Bureau/thesis/articles/Zeng, Breheny - 2017 - The biglasso Package A Memory-and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R.pdf:pdf},
month = {jan},
title = {{The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R}},
url = {http://arxiv.org/abs/1701.05936},
year = {2017}
}
@Manual{R2017,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2017},
    url = {https://www.R-project.org/},
  }
@article{Eddelbuettel2011,
abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, ...) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Eddelbuettel, Dirk and Fran{\c{c}}ois, Romain},
doi = {10.1007/978-1-4614-6868-4},
eprint = {arXiv:1011.1669v3},
isbn = {9781461468677},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {c,call,foreign function interface,r},
pages = {1--18},
pmid = {22057480},
title = {{Rcpp : Seamless R and C ++ Integration}},
url = {http://www.jstatsoft.org/v40/i08/},
volume = {40},
year = {2011}
}
@article{Nielsen2008,
abstract = {BACKGROUND: High-throughput genotyping technology has enabled cost effective typing of thousands of individuals in hundred of thousands of markers for use in genome wide studies. This vast improvement in data acquisition technology makes it an informatics challenge to efficiently store and manipulate the data. While spreadsheets and at text files were adequate solutions earlier, the increased data size mandates more efficient solutions.$\backslash$n$\backslash$nRESULTS: We describe a new binary file format for SNP data, together with a software library for file manipulation. The file format stores genotype data together with any kind of additional data, using a flexible serialisation mechanism. The format is designed to be IO efficient for the access patterns of most multi-locus analysis methods.$\backslash$n$\backslash$nCONCLUSION: The new file format has been very useful for our own studies where it has significantly reduced the informatics burden in keeping track of various secondary data, and where the memory and IO efficiency has greatly simplified analysis runs. A main limitation with the file format is that it is only supported by the very limited set of analysis tools developed in our own lab. This is somewhat alleviated by a scripting interfaces that makes it easy to write converters to and from the format.},
author = {Nielsen, Jesper and Mailund, Thomas},
doi = {10.1186/1471-2105-9-526},
file = {:home/privef/Bureau/thesis/articles/Nielsen, Mailund - 2008 - SNPFile--a software library and file format for large scale association mapping and population genetics studie.pdf:pdf},
isbn = {1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Databases,Genetic,Genetics,Genome-Wide Association Study,Genomics,Genomics: methods,Genotype,Information Storage and Retrieval,Polymorphism,Population,Single Nucleotide,Software},
month = {dec},
number = {1},
pages = {526},
pmid = {19063732},
publisher = {BioMed Central},
title = {{SNPFile--a software library and file format for large scale association mapping and population genetics studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19063732 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2633306 http://www.biomedcentral.com/1471-2105/9/526},
volume = {9},
year = {2008}
}
@article{Euesden2015,
abstract = {SUMMARY: A polygenic risk score (PRS) is a sum of trait-associated alleles across many genetic loci, typically weighted by effect sizes estimated from a genome-wide association study. The application of PRS has grown in recent years as their utility for detecting shared genetic aetiology among traits has become appreciated; PRS can also be used to establish the presence of a genetic signal in underpowered studies, to infer the genetic architecture of a trait, for screening in clinical trials, and can act as a biomarker for a phenotype. Here we present the first dedicated PRS software, PRSice ('precise'), for calculating, applying, evaluating and plotting the results of PRS. PRSice can calculate PRS at a large number of thresholds ("high resolution") to provide the best-fit PRS, as well as provide results calculated at broad P-value thresholds, can thin Single Nucleotide Polymorphisms (SNPs) according to linkage disequilibrium and P-value or use all SNPs, handles genotyped and imputed data, can calculate and incorporate ancestry-informative variables, and can apply PRS across multiple traits in a single run. We exemplify the use of PRSice via application to data on schizophrenia, major depressive disorder and smoking, illustrate the importance of identifying the best-fit PRS and estimate a P-value significance threshold for high-resolution PRS studies.$\backslash$n$\backslash$nAVAILABILITY AND IMPLEMENTATION: PRSice is written in R, including wrappers for bash data management scripts and PLINK-1.9 to minimize computational time. PRSice runs as a command-line program with a variety of user-options, and is freely available for download from http://PRSice.info$\backslash$n$\backslash$nCONTACT: jack.euesden@kcl.ac.uk or paul.oreilly@kcl.ac.uk$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Euesden, Jack and Lewis, Cathryn M and O'Reilly, Paul F},
doi = {10.1093/bioinformatics/btu848},
file = {:home/privef/Bureau/thesis/articles/Euesden, Lewis, O'Reilly - 2015 - PRSice Polygenic Risk Score software.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {14602059},
journal = {Bioinformatics},
month = {may},
number = {9},
pages = {1466--1468},
pmid = {25550326},
publisher = {Oxford University Press},
title = {{PRSice: Polygenic Risk Score software}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25550326 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4410663},
volume = {31},
year = {2015}
}
@article{Vilhjalmsson2015,
abstract = {Polygenic risk scores have shown great promise in predicting complex disease risk and will become more accurate as training sample sizes increase. The standard approach for calculating risk scores involves linkage disequilibrium (LD)-based marker pruning and applying a p value threshold to association statistics, but this discards information and can reduce predictive accuracy. We introduce LDpred, a method that infers the posterior mean effect size of each marker by using a prior on effect sizes and LD information from an external reference panel. Theory and simulations show that LDpred outperforms the approach of pruning followed by thresholding, particularly at large sample sizes. Accordingly, predicted R2 increased from 20.1{\{}{\%}{\}} to 25.3{\{}{\%}{\}} in a large schizophrenia dataset and from 9.8{\{}{\%}{\}} to 12.0{\{}{\%}{\}} in a large multiple sclerosis dataset. A similar relative improvement in accuracy was observed for three additional large disease datasets and for non-European schizophrenia samples. The advantage of LDpred over existing methods will grow as sample sizes increase.},
author = {Vilhj{\'{a}}lmsson, Bjarni J and Yang, Jian and Finucane, Hilary K and Gusev, Alexander and Lindstr{\"{o}}m, Sara and Ripke, Stephan and Genovese, Giulio and Loh, Po-Ru and Bhatia, Gaurav and Do, Ron and Hayeck, Tristan and Won, Hong-Hee and Kathiresan, Sekar and Pato, Michele and Pato, Carlos and Tamimi, Rulla and Stahl, Eli and Zaitlen, Noah and Pasaniuc, Bogdan and Belbin, Gillian and Kenny, Eimear E and Schierup, Mikkel H and {De Jager}, Philip and Patsopoulos, Nikolaos A and McCarroll, Steve and Daly, Mark and Purcell, Shaun and Chasman, Daniel and Neale, Benjamin and Goddard, Michael and Visscher, Peter M and Kraft, Peter and Patterson, Nick and Price, Alkes L},
doi = {10.1016/j.ajhg.2015.09.001},
issn = {00029297},
journal = {The American Journal of Human Genetics},
month = {oct},
number = {4},
pages = {576--592},
pmid = {26430803},
title = {{Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores}},
url = {http://www.sciencedirect.com/science/article/pii/S0002929715003651},
volume = {97},
year = {2015}
}
@article{Marchini2010,
abstract = {In the past few years genome-wide association (GWA) studies have uncovered a large number of convincingly replicated associations for many complex human diseases. Genotype imputation has been used widely in the analysis of GWA studies to boost power, fine-map associations and facilitate the combination of results across studies using meta-analysis. This Review describes the details of several different statistical methods for imputing genotypes, illustrates and discusses the factors that influence imputation performance, and reviews methods that can be used to assess imputation performance and test association at imputed SNPs.;},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02142v2},
author = {Marchini, Jonathan and Howie, Bryan},
doi = {10.1038/nrg2796},
eprint = {arXiv:1507.02142v2},
isbn = {1471-0064},
issn = {1471-0064},
journal = {Nature Reviews. Genetics},
keywords = {Biostatistics/*methods,Genetic,Genome-Wide Association Study*,Genotype,Models,Polymorphism,Single Nucleotide},
month = {jun},
number = {7},
pages = {499--511},
pmid = {20517342},
publisher = {Nature Publishing Group},
title = {{Genotype imputation for genome-wide association studies}},
url = {http://www.nature.com/doifinder/10.1038/nrg2796 http://search.ebscohost.com/login.aspx?direct=true{\&}db=mnh{\&}AN=20517342{\&}lang=fr{\&}site=ehost-live},
volume = {11},
year = {2010}
}
@article{Aulchenko2010,
abstract = {Over the last few years, genome-wide association (GWA) studies became a tool of choice for the identification of loci associated with complex traits. Currently, imputed single nucleotide polymorphisms (SNP) data are frequently used in GWA analyzes. Correct analysis of imputed data calls for the implementation of specific methods which take genotype imputation uncertainty into account.},
author = {Aulchenko, Yurii S and Struchalin, Maksim V and van Duijn, Cornelia M},
doi = {10.1186/1471-2105-11-134},
issn = {1471-2105},
journal = {BMC Bioinformatics},
month = {mar},
number = {1},
pages = {134},
title = {{ProbABEL package for genome-wide association analysis of imputed data}},
url = {http://dx.doi.org/10.1186/1471-2105-11-134},
volume = {11},
year = {2010}
}
@article{Abraham2016a,
abstract = {$\backslash$n          Principal component analysis (PCA) is a crucial step in quality control of genomic data and a common approach for understanding population genetic structure. With the advent of large genotyping studies involving hundreds of thousands of individuals, standard approaches are no longer feasible. However, when the full decomposition is not required, substantial computational savings can be made. We present FlashPCA2, a tool that can perform partial PCA on 1 million individuals faster than competing approaches, while requiring substantially less memory.$\backslash$n          https://github.com/gabraham/flashpca.$\backslash$n          gad.abraham@unimelb.edu.au.$\backslash$n          Supplementary data are available at Bioinformatics online.$\backslash$n        },
author = {Abraham, Gad and Qiu, Yixuan and Inouye, Michael},
doi = {10.1101/094714},
file = {:home/privef/Bureau/thesis/articles/Abraham, Qiu, Inouye - 2016 - FlashPCA2 principal component analysis of biobank-scale genotype datasets.pdf:pdf},
issn = {1367-4803},
journal = {bioRxiv},
month = {may},
pages = {2014--2017},
pmid = {28475694},
title = {{FlashPCA2 : principal component analysis of biobank-scale genotype datasets}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btx299},
volume = {12},
year = {2016}
}
@article{Abraham2014a,
abstract = {Principal component analysis (PCA) is routinely used to analyze genome-wide single-nucleotide polymorphism (SNP) data, for detecting population structure and potential outliers. However, the size of SNP datasets has increased immensely in recent years and PCA of large datasets has become a time consuming task. We have developed flashpca, a highly efficient PCA implementation based on randomized algorithms, which delivers identical accuracy in extracting the top principal components compared with existing tools, in substantially less time. We demonstrate the utility of flashpca on both HapMap3 and on a large Immunochip dataset. For the latter, flashpca performed PCA of 15,000 individuals up to 125 times faster than existing tools, with identical results, and PCA of 150,000 individuals using flashpca completed in 4 hours. The increasing size of SNP datasets will make tools such as flashpca essential as traditional approaches will not adequately scale. This approach will also help to scale other applications that leverage PCA or eigen-decomposition to substantially larger datasets.},
author = {Abraham, Gad and Inouye, Michael},
doi = {10.1371/journal.pone.0093766},
editor = {Zhang, Yu},
file = {:home/privef/Bureau/thesis/articles/Abraham, Inouye - 2014 - Fast principal component analysis of large-scale genome-wide data.pdf:pdf},
isbn = {10.1371/journal.pone.0093766},
issn = {19326203},
journal = {PLoS ONE},
month = {apr},
number = {4},
pages = {e93766},
pmid = {24718290},
publisher = {Public Library of Science},
title = {{Fast principal component analysis of large-scale genome-wide data}},
url = {http://dx.plos.org/10.1371/journal.pone.0093766},
volume = {9},
year = {2014}
}
@article{Galinsky2016,
abstract = {Searching for genetic variants with unusual differentiation between subpopulations is an established approach for identifying signals of natural selection. However, existing methods generally require discrete subpopulations. We introduce a method that infers selection using principal components (PCs) by identifying variants whose differentiation along top PCs is significantly greater than the null distribution of genetic drift. To enable the application of this method to large datasets, we developed the FastPCA software, which employs recent advances in random matrix theory to accurately approximate top PCs while reducing time and memory cost from quadratic to linear in the number of individuals, a computational improvement of many orders of magnitude. We apply FastPCA to a cohort of 54,734 European Americans, identifying 5 distinct subpopulations spanning the top 4 PCs. Using the PC-based test for natural selection, we replicate previously known selected loci and identify three new genome-wide significant signals of selection, including selection in Europeans at ADH1B. The coding variant rs1229984aT has previously been associated to a decreased risk of alcoholism and shown to be under selection in East Asians; we show that it is a rare example of independent evolution on two continents. We also detect selection signals at IGFBP3 and IGH, which have also previously been associated to human disease.},
author = {Galinsky, Kevin J. and Bhatia, Gaurav and Loh, Po Ru and Georgiev, Stoyan and Mukherjee, Sayan and Patterson, Nick J. and Price, Alkes L.},
doi = {10.1016/j.ajhg.2015.12.022},
file = {:home/privef/Bureau/thesis/articles/Galinsky et al. - 2016 - Fast Principal-Component Analysis Reveals Convergent Evolution of ADH1B in Europe and East Asia.pdf:pdf},
isbn = {1537-6605 (Electronic)$\backslash$r0002-9297 (Linking)},
issn = {15376605},
journal = {American Journal of Human Genetics},
number = {3},
pages = {456--472},
pmid = {26924531},
title = {{Fast Principal-Component Analysis Reveals Convergent Evolution of ADH1B in Europe and East Asia}},
volume = {98},
year = {2016}
}
@article{Price2006,
abstract = {Population stratification--allele frequency differences between cases and controls due to systematic ancestry differences-can cause spurious associations in disease studies. We describe a method that enables explicit detection and correction of population stratification on a genome-wide scale. Our method uses principal components analysis to explicitly model ancestry differences between cases and controls. The resulting correction is specific to a candidate marker's variation in frequency across ancestral populations, minimizing spurious associations while maximizing power to detect true associations. Our simple, efficient approach can easily be applied to disease studies with hundreds of thousands of markers.},
author = {Price, Alkes L and Patterson, Nick J and Plenge, Robert M and Weinblatt, Michael E and Shadick, Nancy A and Reich, David},
doi = {10.1038/ng1847},
file = {:home/privef/Bureau/thesis/articles/Price et al. - 2006 - Principal components analysis corrects for stratification in genome-wide association studies.pdf:pdf},
issn = {1061-4036},
journal = {Nature genetics},
keywords = {Algorithms,Alleles,Case-Control Studies,Databases,Genetic Markers,Genome,Genomics,Genomics: statistics {\&} numerical data,Genotype,Human,Humans,Nucleic Acid,Phenotype,Polymorphism,Principal Component Analysis,Single Nucleotide},
month = {aug},
number = {8},
pages = {904--9},
pmid = {16862161},
shorttitle = {Nat Genet},
title = {{Principal components analysis corrects for stratification in genome-wide association studies.}},
url = {http://dx.doi.org/10.1038/ng1847},
volume = {38},
year = {2006}
}
@article{Gogarten2012,
abstract = {GWASTools is an R/Bioconductor package for quality control and analysis of genome-wide association studies (GWAS). GWASTools brings the interactive capability and extensive statistical libraries of R to GWAS. Data are stored in NetCDF format to accommodate extremely large datasets that cannot fit within R's memory limits. The documentation includes instructions for converting data from multiple formats, including variants called from sequencing. GWASTools provides a convenient interface for linking genotypes and intensity data with sample and single nucleotide polymorphism annotation.},
author = {Gogarten, Stephanie M. and Bhangale, Tushar and Conomos, Matthew P. and Laurie, Cecelia A. and McHugh, Caitlin P. and Painter, Ian and Zheng, Xiuwen and Crosslin, David R. and Levine, David and Lumley, Thomas and Nelson, Sarah C. and Rice, Kenneth and Shen, Jess and Swarnkar, Rohit and Weir, Bruce S. and Laurie, Cathy C.},
doi = {10.1093/bioinformatics/bts610},
file = {:home/privef/Bureau/thesis/articles/Gogarten et al. - 2012 - GWASTools an RBioconductor package for quality control and analysis of genome-wide association studies.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
month = {dec},
number = {24},
pages = {3329--3331},
pmid = {23052040},
publisher = {Oxford University Press},
title = {{GWASTools: An R/Bioconductor package for quality control and analysis of genome-wide association studies}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bts610},
volume = {28},
year = {2012}
}
@article{Purcell2007,
abstract = {Whole-genome association studies (WGAS) bring new computational, as well as analytic, challenges to researchers. Many existing genetic-analysis tools are not designed to handle such large data sets in a convenient manner and do not necessarily exploit the new opportunities that whole-genome data bring. To address these issues, we developed PLINK, an open-source C/C++ WGAS tool set. With PLINK, large data sets comprising hundreds of thousands of markers genotyped for thousands of individuals can be rapidly manipulated and analyzed in their entirety. As well as providing tools to make the basic analytic steps computationally efficient, PLINK also supports some novel approaches to whole-genome data that take advantage of whole-genome coverage. We introduce PLINK and describe the five main domains of function: data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation. In particular, we focus on the estimation and use of identity-by-state and identity-by-descent information in the context of population-based whole-genome studies. This information can be used to detect and correct for population stratification and to identify extended chromosomal segments that are shared identical by descent between very distantly related individuals. Analysis of the patterns of segmental sharing has the potential to map disease loci that contain multiple rare variants in a population-based linkage analysis.},
author = {Purcell, Shaun and Neale, Benjamin and Todd-Brown, Kathe and Thomas, Lori and Ferreira, Manuel A R and Bender, David and Maller, Julian and Sklar, Pamela and de Bakker, Paul I W and Daly, Mark J and Sham, Pak C},
doi = {10.1086/519795},
file = {:home/privef/Bureau/thesis/articles/Purcell et al. - 2007 - PLINK a tool set for whole-genome association and population-based linkage analyses.pdf:pdf},
issn = {0002-9297},
journal = {American journal of human genetics},
keywords = {Genetic Linkage,Genetic Linkage: genetics,Genome,Human,Human: genetics,Humans,Polymorphism,Population,Population: genetics,Single Nucleotide,Software},
language = {English},
month = {sep},
number = {3},
pages = {559--75},
pmid = {17701901},
publisher = {Elsevier},
title = {{PLINK: a tool set for whole-genome association and population-based linkage analyses.}},
url = {http://www.cell.com/article/S0002929707613524/fulltext},
volume = {81},
year = {2007}
}
@article{Li2011,
abstract = {MOTIVATION: Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification. RESULTS: In this article, we present a Support Vector Machine classifier that can correct the prediction for observed confounding factors. This is achieved by minimizing the statistical dependence between the classifier and the confounding factors. We prove that this formulation can be transformed into a standard Support Vector Machine with rescaled input data. In our experiments, our confounder correcting SVM (ccSVM) improves tumor diagnosis based on samples from different labs, tuberculosis diagnosis in patients of varying age, ethnicity and gender, and phenotype prediction in the presence of population structure and outperforms state-of-the-art methods in terms of prediction accuracy. AVAILABILITY: A ccSVM-implementation in MATLAB is available from http://webdav.tuebingen.mpg.de/u/karsten/Forschung/ISMB11{\_}ccSVM/. CONTACT: limin.li@tuebingen.mpg.de; karsten.borgwardt@tuebingen.mpg.de.},
author = {Li, Limin and Rakitsch, Barbara and Borgwardt, Karsten},
doi = {10.1093/bioinformatics/btr204},
file = {:home/privef/Bureau/thesis/articles/Li, Rakitsch, Borgwardt - 2011 - ccSVM correcting Support Vector Machines for confounding factors in biological data classification.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Acute,Acute: diagnosis,Acute: epidemiology,Acute: genetics,Age Factors,Area Under Curve,Artificial Intelligence,Computational Biology,Computational Biology: methods,Humans,Leukemia, Myeloid, Acute,Leukemia, Myeloid, Acute: diagnosis,Leukemia, Myeloid, Acute: epidemiology,Leukemia, Myeloid, Acute: genetics,Microarray Analysis,Myeloid,Phenotype,Plants,Plants: genetics,Sex Factors,Tuberculosis,Tuberculosis: diagnosis,Tuberculosis: epidemiology},
month = {jul},
number = {13},
pages = {i342--8},
pmid = {21685091},
title = {{ccSVM: correcting Support Vector Machines for confounding factors in biological data classification.}},
url = {http://bioinformatics.oxfordjournals.org/content/27/13/i342.full},
volume = {27},
year = {2011}
}
@article{Danecek2011,
abstract = {SUMMARY: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API.$\backslash$n$\backslash$nAVAILABILITY: http://vcftools.sourceforge.net},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard},
doi = {10.1093/bioinformatics/btr330},
eprint = {NIHMS150003},
file = {:home/privef/Bureau/thesis/articles/Danecek et al. - 2011 - The variant call format and VCFtools.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
month = {aug},
number = {15},
pages = {2156--2158},
pmid = {21653522},
publisher = {Oxford University Press},
title = {{The variant call format and VCFtools}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr330},
volume = {27},
year = {2011}
}
@article{Kane2013,
abstract = {This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the foreach package allows users of the R programming environment to define parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platformspecific code. Second, the bigmemory package implements memoryand filemapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.},
author = {Kane, Michael J and Emerson, John W and Weston, Stephen},
doi = {10.18637/jss.v055.i14},
file = {:home/privef/Bureau/thesis/articles/Kane, Emerson, Weston - 2013 - Scalable Strategies for Computing with Massive Data.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
number = {14},
pages = {1--19},
title = {{Scalable Strategies for Computing with Massive Data}},
url = {http://www.jstatsoft.org/v55/i14/},
volume = {55},
year = {2013}
}

@article{mccarthy2016reference,
  title={A reference panel of 64,976 haplotypes for genotype imputation},
  author={McCarthy, Shane and Das, Sayantan and Kretzschmar, Warren and Delaneau, Olivier and Wood, Andrew R and Teumer, Alexander and Kang, Hyun Min and Fuchsberger, Christian and Danecek, Petr and Sharp, Kevin and others},
  journal={Nature genetics},
  volume={48},
  number={10},
  pages={1279},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{hayward2016complex,
  title={Complex disease and phenotype mapping in the domestic dog},
  author={Hayward, Jessica J and Castelhano, Marta G and Oliveira, Kyle C and Corey, Elizabeth and Balkman, Cheryl and Baxter, Tara L and Casal, Margret L and Center, Sharon A and Fang, Meiying and Garrison, Susan J and others},
  journal={Nature communications},
  volume={7},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{sikorska2013gwas,
  title={GWAS on your notebook: fast semi-parallel linear and logistic regression for genome-wide association studies},
  author={Sikorska, Karolina and Lesaffre, Emmanuel and Groenen, Patrick FJ and Eilers, Paul HC},
  journal={BMC bioinformatics},
  volume={14},
  number={1},
  pages={166},
  year={2013},
  publisher={BioMed Central}
}

@article{chang2015second,
  title={Second-generation PLINK: rising to the challenge of larger and richer datasets},
  author={Chang, Christopher C and Chow, Carson C and Tellier, Laurent CAM and Vattikuti, Shashaank and Purcell, Shaun M and Lee, James J},
  journal={Gigascience},
  volume={4},
  number={1},
  pages={7},
  year={2015},
  publisher={BioMed Central}
}

@article{zheng2012high,
  title={A high-performance computing toolset for relatedness and principal component analysis of SNP data},
  author={Zheng, Xiuwen and Levine, David and Shen, Jess and Gogarten, Stephanie M and Laurie, Cathy and Weir, Bruce S},
  journal={Bioinformatics},
  volume={28},
  number={24},
  pages={3326--3328},
  year={2012},
  publisher={Oxford University Press}
}
